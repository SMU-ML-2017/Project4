{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model\n",
    "\n",
    "matplotlib.rcParams.update({'figure.figsize': (15, 9)})\n",
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "matplotlib.rcParams.update({'axes.labelsize': 20})\n",
    "matplotlib.rcParams.update({'xtick.labelsize': 12})\n",
    "matplotlib.rcParams.update({'ytick.labelsize': 12})\n",
    "matplotlib.rcParams.update({'font.family': 'Helvetica, Arial, sans-serif'})\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning lab 4: extending logistic regression\n",
    "## Jake Rowland and Paul Herz\n",
    "2017-10-01\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Here we return to the dataset of our initial project, [Exploring Table Data](https://github.com/SMU-ML-2017/Project1/blob/master/ML%20Lab%201.ipynb), wherein we graphically analyzed the trends in the [IMDB Top 5000 Movie Dataset](https://www.kaggle.com/deepmatrix/imdb-5000-movie-dataset). One aspect of our analysis in that project centered around dividing the set of films into three groups: **poor**, **average**, and **good**. This was a simplification of IMDB's 10-point rating system, and grouped films into asymmetric quantiles. It allowed us to cut arbitrary lines in the large group of films to illustrate trends among the upper percentile films versus the rest. Here, we will try to use several classification techniques to predict these three classes, and compare the results. One technique will involve a handspun implementation to display the inner workings of such an algorithm.\n",
    "\n",
    "### 1.1 Background\n",
    "\n",
    "*Explain the task and what business-case or use-case it is designed to solve (or designed to investigate). Detail exactly what the classification task is and what parties would be interested in the results.*\n",
    "\n",
    "This dataset represents data compiled entirely by IMDB, whose primary purpose is as a compendium of films. In the previous use of this dataset, we mostly considered how this dataset serves as a representation of trends in American and international cinema. This time, we want to consider the films more in the context of IMDB as an application, and the trends of American film consumers.\n",
    "\n",
    "### 1.2 Business Case\n",
    "\n",
    "Using the same arbitrary classifications of **poor**, **average**, and **good**, whose definitions depend on the quantile groupings of IMDB out-of-10 scores, we want to use film data (less the scores) to predict a film's classification into one of these groups. We believe that such a feature will be useful to IMDB, which is likely to receive a sizeable portion of traffic from visitors looking up brand-new films. However, brand-new films do not have sufficient reviews for IMDB to formulate their composite out-of-ten score, and must display a placeholder until there is consensus among a statistically significant number of film critics. Instead of leaving users dissatisfied when they cannot quickly determine whether they should see the film, or how it is performing, IMDB can classify the movie as \"probably bad,\" \"probably average,\" or \"probably good.\" This is not as a replacement to critic composites, but to fill in the blank before reviews come in. \n",
    "\n",
    "This capability, to preemptively label new material based on relevant input data like directors, budgets, etc. is of great value to IMDB, and although there is a definite threshold of performance required for meaningful utility of the prediction task, there is no risk presented to IMDB. In the worst-case situation, IMDB provides a preliminary rating that is not quite in line with the forthcoming critics' reviews. What would befall IMDB is akin to what would befall the publisher of a critic whose review is an outlier—absolutely nothing. In the best-case situation, this preemptive rating adds value to the IMDB platform as it advises moviegoers on the probable quality of brand-new films, which will increase traffic and therefore advertisement revenue.\n",
    "\n",
    "### 1.3 Serviceability\n",
    "\n",
    "As it was mentioned, this task and its implementation as a value-adding tool in the IMDB interface presents virtually no risk to IMDB, even if it were to perform poorly. However, there is a definite point in the range of possible performance at which the product becomes *useful*. We believe that our model must be 80% accurate to gain public trust when our preemptive ratings are referred to — a 1 in 5 chance of predicting a movie to be poor/average/good when it is not. This number may seem arbitrary, but it is our best approximation of an error rate that an end user would be willing to tolerate.\n",
    "\n",
    "## 2. The dataset: preprocessing and review\n",
    "\n",
    "### 2.1 Dataset preparation\n",
    "\n",
    "Below, we load the dataset and reorder columns in a more reasonable fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "m = pd.read_csv('movie_metadata.csv')\n",
    "\n",
    "# Reorder the DataFrame to a more intelligent fashion\n",
    "m = m[[\n",
    "    'movie_title','title_year',\n",
    "    'genres', 'plot_keywords', 'duration',\n",
    "    'budget', 'gross',\n",
    "    'language', 'country', 'content_rating',\n",
    "    'color', 'aspect_ratio',\n",
    "    'facenumber_in_poster',\n",
    "    'director_name',\n",
    "    'actor_1_name', 'actor_2_name', 'actor_3_name',\n",
    "    'movie_facebook_likes', 'director_facebook_likes', 'actor_1_facebook_likes', 'actor_2_facebook_likes',\n",
    "    'actor_3_facebook_likes', 'cast_total_facebook_likes',\n",
    "    'movie_imdb_link', 'num_user_for_reviews', 'num_critic_for_reviews', 'num_voted_users',\n",
    "    'imdb_score',\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we cast integral data columns out of the default float type, to ensure the data is best represented by the right type. We follow by removing duplicates, and then copying the dataset before further destructive preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the number of float64 data types for columns that do not need a float64 data type\n",
    "for col in ['title_year','facenumber_in_poster',\n",
    "'movie_facebook_likes','actor_1_facebook_likes','actor_2_facebook_likes',\n",
    "'actor_3_facebook_likes','cast_total_facebook_likes','num_user_for_reviews',\n",
    "'num_critic_for_reviews','num_voted_users']:\n",
    "    m[col] = pd.to_numeric(m[col],downcast='integer')\n",
    "  \n",
    "# Remove all duplicate entries\n",
    "m.drop_duplicates(inplace=True)\n",
    "\n",
    "# Create a copy to perserve the original DataFrame\n",
    "m_original=m.copy()\n",
    "m.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data quality\n",
    "\n",
    "As described in our prior report on this dataset, it presents numerous data consistency and quality challenges to be overcome before it can be useful for analysis. One problem that we inspected and demonstrated in the prior report was as follows: some films were massive outliers on the budget and gross scale, one film in particular making an absurd amount of money: Studio Ghibli's *Princess Mononoke*. We posited that this may have been because it was in Japanese Yen (JPY), a much smaller unit of currency than the US Dollar (USD), and a quick verification of the IMDB website confirmed our suspicions. Unfortunately, this dataset stores all financial quantities as bare numbers, and there is no reasonable means of guessing which currency is being referred to. Our best solution to avoid this massive inaccuracy was to remove all non-US films, working off the reasonable assumption that all US films would have financial figures reported in USD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all films except those from the United States\n",
    "m = m[m['country'] == 'USA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another inconsistency arose when we realized that, unfortunately, a number of television series and other non-cinematic formats were being included in this list. This may be an oversight of those preparing this dataset from the IMDB corpus, as the dataset is supposedly \"5000 Movies,\" and nothing more. One challenge presented by the mixture of media formats is the inability to compare homogeneous instances—for example, if we were to use running time as a metric in our prediction task, TV series would represent a large outlier, as IMDB lists the series' *full running times* of all episodes. In fact, this was how the issue of non-cinematic instances being present in the dataset was initially uncovered in preliminary analysis during our prior report. In order to filter such content out, we've found the most reliable means to be removing those items which do not feature a standard, common, and mainstream content explicitness rating: G, PG, PG-13, or R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove items with non-American or non-film rating systems\n",
    "m = m[m['content_rating'].isin(['R','PG-13','PG','G'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address the number of films containing null fields, we use common-sense and statistics to handle inconsistent instances. In the case of most fields, we remove instances with missing columns. The reasoning behind this is that the fields we remove for represent fairly common data, and a film missing this data may be very esoteric or poorly documented in IMDB to miss it. We believe Facebook likes are very likely to correlate with ratings in the case of some larger modern movies, so we strip those films which lack Facebook data for film Fan pages and actor individual profiles.\n",
    "\n",
    "Other fields can be imputed: we assume a lack of a review count means there are zero reviews, and we use the mean of all faces in film posters to replace null values for `facenumber_in_poster`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where the value is null\n",
    "for col in ['title_year', 'language','country','content_rating',\n",
    "'aspect_ratio','duration', 'color','gross','budget','movie_facebook_likes',\n",
    "'actor_1_facebook_likes','actor_2_facebook_likes','actor_3_facebook_likes',\n",
    "'cast_total_facebook_likes']:\n",
    "    try:\n",
    "        m = m[pd.notnull(m[col])]\n",
    "    except KeyError:\n",
    "        pass\n",
    "    \n",
    "# Assume null review counts are 0\n",
    "for col in ['num_user_for_reviews','num_critic_for_reviews']:\n",
    "    m[col].fillna(value=0,inplace=True)\n",
    "    \n",
    "# Assume missing face counts are the mean\n",
    "avgFace = round(m['facenumber_in_poster'].mean())\n",
    "m['facenumber_in_poster'].fillna(value=avgFace, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arbitrary class generation: the three rating classes\n",
    "\n",
    "Below, we generate the arbitrary partitions of the dataset along the IMDB out-of-10 scores. This will simplify the classification process beyond the theoretically 100 possible scores (0.0-10.0 in increments of 1) or the 10 possible scores if we were to round or truncate to whole numbers. First we determine the boundaries along the range of possible scores at which we will partition the dataset (`poor_avg` and `avg_good`). These are based off of the 50th percentile point and the 90th percentile point, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize the IMDB score into three classes:\n",
    "# [0%-49%] is Poor, [50%-89%] is Average, [90%-100%] is Good.\n",
    "poor_avg = m['imdb_score'].quantile(.5)\n",
    "avg_good = m['imdb_score'].quantile(.9)\n",
    "\n",
    "m['rating_category'] = pd.cut(\n",
    "    m.imdb_score,[0,poor_avg,avg_good,10],\n",
    "    labels=['poor','average','good']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing irrelevant fields\n",
    "\n",
    "We will follow by removing columns that are unreasonable to categorize (e.g. names and multi-value genres), and those irrelevant to classification (IMDB page URLs). Now that we have sorted films by IMDB score into rating classes, we also remove the IMDB score field. Country and language are removed as we have limited films to the United States, and the list of possible languages is too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ['movie_title','plot_keywords','actor_1_name','actor_2_name','actor_3_name',\n",
    "'movie_imdb_link','genres', 'director_name','imdb_score','aspect_ratio','country','language']:\n",
    "    m.drop(c, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make `content_rating` and `color` categorical, as their possible values are small, and they may correlate meaningfully to certain rating classes. This is a preliminary step before one-hot encoding necessary for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical values to category type\n",
    "for col in ['content_rating','color']:\n",
    "    m[col] = m[col].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Modifying the dataset for classification compatibility\n",
    "\n",
    "What has been done to the dataset so far mostly resembles the preprocessing we performed in our prior report touching on this same dataset. What follows represents the additional processing necessary for the classification we will be performing.\n",
    "\n",
    "First, categorical types such as `color` and `content_rating` are not at all useful to a classifier. We will convert these non-ordinal types into one-hot columns, due to the limited number of categories in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2963 entries, 0 to 5042\n",
      "Data columns (total 21 columns):\n",
      "title_year                   2963 non-null float64\n",
      "duration                     2963 non-null float64\n",
      "budget                       2963 non-null float64\n",
      "gross                        2963 non-null float64\n",
      "facenumber_in_poster         2963 non-null float64\n",
      "movie_facebook_likes         2963 non-null int32\n",
      "director_facebook_likes      2963 non-null float64\n",
      "actor_1_facebook_likes       2963 non-null float64\n",
      "actor_2_facebook_likes       2963 non-null float64\n",
      "actor_3_facebook_likes       2963 non-null float64\n",
      "cast_total_facebook_likes    2963 non-null int32\n",
      "num_user_for_reviews         2963 non-null float64\n",
      "num_critic_for_reviews       2963 non-null float64\n",
      "num_voted_users              2963 non-null int32\n",
      "rating_category              2963 non-null category\n",
      "color_ Black and White       2963 non-null uint8\n",
      "color_Color                  2963 non-null uint8\n",
      "content_rating_G             2963 non-null uint8\n",
      "content_rating_PG            2963 non-null uint8\n",
      "content_rating_PG-13         2963 non-null uint8\n",
      "content_rating_R             2963 non-null uint8\n",
      "dtypes: category(1), float64(11), int32(3), uint8(6)\n",
      "memory usage: 332.9 KB\n"
     ]
    }
   ],
   "source": [
    "# replace color and content_rating with dummies\n",
    "m = pd.get_dummies(m, columns=['color','content_rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since rating category (poor, average, good) is both (1) our target for classification in a one-versus-rest format, and (2) an ordinal value (poor < average < good), we convert the rating category field not to a one-hot set, but an enumerated integer code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1456\n",
       "1    1231\n",
       "2     276\n",
       "Name: rating_category, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if m.rating_category.dtype != np.dtype('int8'):\n",
    "    m.rating_category = m.rating_category.cat.codes\n",
    "m.rating_category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prepare the material which will be used directly in our classification task. Here, `X` represents all instances, less the target variable `rating_category`, whereas `y` is a vector of the target variable for all instances.\n",
    "\n",
    "We perform a nonrandom split of the dataset for simplicity's sake, but this could easily be replaced with a more robust sampling mechanism. `X_train` is 80% of the instance data for training, whereas `X_predict` is the testing set. `y_train` and `y_predict` correspond, but `y_predict` is only used for accuracy calculation, and does not feed back into training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=m.drop('rating_category', axis=1, inplace=False)\n",
    "y=np.ravel(m['rating_category'])\n",
    "\n",
    "split_index = int(len(m)*0.8)\n",
    "X_train = X[:split_index]\n",
    "X_predict = X[split_index:]\n",
    "y_train = y[:split_index]\n",
    "y_predict = y[split_index:]\n",
    "\n",
    "type(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted 1    369\n",
      "0    200\n",
      "2     24\n",
      "dtype: int64\n",
      "Actual 1    285\n",
      "0    258\n",
      "2     50\n",
      "dtype: int64\n",
      "[[139  60   1]\n",
      " [119 220  30]\n",
      " [  0   5  19]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  2.70670632e-06,   9.29206545e-08,   1.94725562e-08,\n",
       "          4.26883064e-09,   3.03963961e-09,  -7.88845810e-06,\n",
       "         -1.67991398e-06,   1.22319154e-06,   2.05359998e-07,\n",
       "          7.82966274e-07,   3.77450250e-06,   1.93695656e-07,\n",
       "          7.69562936e-08,  -1.52688185e-05,  -6.03054113e-11,\n",
       "          1.40916377e-09,  -5.27423771e-11,  -7.76609739e-11,\n",
       "          1.10956127e-09,   3.69700441e-10],\n",
       "       [ -2.87471917e-06,  -1.04729004e-07,  -6.69589931e-09,\n",
       "          2.53262363e-09,  -2.91174761e-09,   5.04803943e-06,\n",
       "          2.30343394e-07,   3.44911795e-06,   3.21321331e-07,\n",
       "         -6.45138288e-07,   1.41261685e-06,  -2.55820801e-08,\n",
       "          5.51190949e-08,  -1.04373501e-06,   4.22430355e-11,\n",
       "         -1.47854414e-09,  -4.18235738e-11,  -2.87056494e-10,\n",
       "         -1.02639123e-09,  -8.10298036e-11],\n",
       "       [ -1.43173340e-03,  -6.47588820e-05,  -1.81908834e-08,\n",
       "         -4.42616069e-09,  -1.24294533e-06,   7.06268752e-06,\n",
       "          3.56130656e-05,   7.91656450e-05,   1.39059951e-04,\n",
       "         -7.03459678e-05,  -1.02271532e-04,  -9.99571956e-05,\n",
       "         -8.38765143e-05,   1.35085118e-05,  -9.70402664e-09,\n",
       "         -7.04283245e-07,   1.13752742e-08,  -5.39198695e-08,\n",
       "         -2.82858715e-07,  -3.88583961e-07]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic = linear_model.LogisticRegression(multi_class='ovr')\n",
    "logistic.fit(X_train,y_train)\n",
    "C = logistic.predict(X_predict)\n",
    "print('Predicted', pd.Series(C).value_counts())\n",
    "print('Actual', pd.Series(y_predict).value_counts())\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "m = confusion_matrix(C,y_predict)\n",
    "print(m)\n",
    "\n",
    "logistic.coef_\n",
    "\n",
    "#  Default\n",
    "# [[130 156  38]\n",
    "#  [ 85  98  10]\n",
    "#  [ 43  31   2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "\n",
    "class Regression:\n",
    "\n",
    "    def __init__(self, eta, maxIter=20, C=0.001, opt='GD', reg ='NA'):\n",
    "        self.eta = eta\n",
    "        self.maxIter = maxIter\n",
    "        self.C = C\n",
    "        self.optimization = opt\n",
    "        self.regularization = reg\n",
    "        \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        # ydiff = y(m,1) - yHat(m,1)\n",
    "        # The ravel of ydiff is 1-D array of size m\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        # X is (m,n+1)\n",
    "        # ydiff is 1-D array of size m. ydiff[:,np.newaxis] is matrix of (m,1)\n",
    "        # X * ydiff[:,np.newaxis] is matrix (m,n+1) where each row is X[m] @ ydiff\n",
    "        # Calculate the horizontal average for each row. Result is 1-D array of size m\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        # Reshape the gradient to be matrix (m,1)\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        \n",
    "        if(self.regularization == 'L1'):\n",
    "            # L1 = C * sum(|w|)\n",
    "            gradient[1:] += self.C * np.sum(abs(self.w_[1:])) \n",
    "        elif(self.regularization == 'L2'):\n",
    "            # L2 = C * sum(|w|^2)\n",
    "            gradient[1:] += np.sum(self.w_[1:]*self.w_[1:]) * self.C\n",
    "            #gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        elif(self.regularization == 'L1/L2'):\n",
    "            # DOES NOT WORK\n",
    "            #Calculate the L1 and L2 regularization\n",
    "            L1 = np.sum(abs(self.w_[1:]))\n",
    "            L2 = self.w_[1:]*self.w_[1:]\n",
    "            # Mutlipy the quotient of L1 and L2 by regularization rate\n",
    "            gradient[1:] += np.divide(L1,L2) * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    def _gradient_descent(self):\n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.maxIter):\n",
    "            # Get the gradient of form (m,1)\n",
    "            gradient = self._get_gradient(self.Xb,self.y)\n",
    "            # Add the associated gradient to the weight to adjust the weights.\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate\n",
    "            \n",
    "            \n",
    "    def _stocastic_descent(self):\n",
    "        for _ in range(self.maxIter):\n",
    "            for m in range(1,self.num_samples):\n",
    "                # Calulate the weights for 1st instance, then first 2, then first 3..., then first m instances\n",
    "                gradient = self._get_gradient(self.Xb[:m], self.y[:m])\n",
    "                self.w_ += gradient*self.eta # multipy by learning rate\n",
    "    \n",
    "            \n",
    "    def _newton_H(self,X,y):\n",
    "        # Get P(y=1|x,w_)\n",
    "        hX = self.predict_proba(X,add_bias=False)\n",
    "        # get P(y=0|x,w_)\n",
    "        one_hX = 1 - hX\n",
    "        \n",
    "        # Multiply together\n",
    "        prob_Mult = hX*one_hX\n",
    "        \n",
    "        # Create matrix to perform average on\n",
    "        H = np.zeros((X.shape[1], X.shape[1]))\n",
    "        for row in range(X.shape[0]):\n",
    "            xi = X[row] # Get the row of X\n",
    "            xi.shape = (X.shape[1],1) #Convert to 2D array\n",
    "            # Get the transpose of the row\n",
    "            xiT = xi.T\n",
    "            \n",
    "            # Create a matrix from x and xT\n",
    "            xMat = xi*xiT\n",
    "            \n",
    "            # Scalar multipy the probibility multiplyer \n",
    "            # and the xMatrix\n",
    "            xMat = prob_Mult[row] * xMat\n",
    "            \n",
    "            # Add the matrix to accumulation matrix\n",
    "            H = H + xMat\n",
    "            \n",
    "        # Divide the accumulation matrix by the number of samples to get average\n",
    "        H = H / X.shape[0]\n",
    "        \n",
    "        return H\n",
    "    \n",
    "        \n",
    "    def _newton_method(self):\n",
    "        for _ in range(self.maxIter):\n",
    "            # Get the hessian matrix (Second derivative)\n",
    "            H = self._newton_H(self.Xb, self.y)\n",
    "            # Get the gradient (First Derivative)\n",
    "            gradient = self._get_gradient(self.Xb, self.y)\n",
    "            try:\n",
    "                # Attempt to inverse the hessian\n",
    "                H_inv = np.linalg.inv(H)\n",
    "            \n",
    "                # Modify the w_(t+1) = w_(t) - (dJ(w_))/(d^2J(w_))\n",
    "                self.w_ -= (H_inv @ gradient)\n",
    "            except:\n",
    "                print('Singularity Matrix')\n",
    "    \n",
    "    # public:\n",
    "    # Takes (m,n) matrix - X. Add bias term to X matrix to create (m,n+1) matrix - Xb\n",
    "    # Calculate the dot product of Xb(m,n+1) and w_(n+1,1) -> result is matrix of (m,1)\n",
    "    # Calculate the sigmoid for each value of the dot product and return a matrix of (m,1)\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X,prob=.5):\n",
    "        return (self.predict_proba(X) >= prob)\n",
    "    \n",
    "    def predict_raw(self,X):\n",
    "        return (self.predict_proba(X))\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.Xb = self._add_bias(X) # add bias term\n",
    "        self.num_samples, self.num_features = self.Xb.shape\n",
    "        self.y = y\n",
    "        \n",
    "        self.w_ = np.zeros((self.num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # Select the optimization process\n",
    "        if(self.optimization == 'GD'):\n",
    "            self._gradient_descent()\n",
    "        elif (self.optimization == 'SGD'):\n",
    "            self._stocastic_descent()\n",
    "        elif (self.optimization == 'NWT'):\n",
    "            self._newton_method()\n",
    "        else:\n",
    "            self._gradient_descent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassRegression:\n",
    "    \n",
    "    def __init__(self, eta, maxIter=20, C=0.001, opt='GD', reg ='NA'):\n",
    "        self.eta = eta\n",
    "        self.maxIter = maxIter\n",
    "        self.C = C\n",
    "        self.optimization = opt\n",
    "        self.regularization = reg\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.true_y = y\n",
    "        \n",
    "        # Find unique classes\n",
    "        self.classes = np.unique(y).tolist()\n",
    "        \n",
    "        #Create dictonary to hold regression class associated with the class as the key\n",
    "        self.reg = {}\n",
    "        \n",
    "        # Define training classes as matrix of (n_class, n_samples)\n",
    "        self.y_classes = np.zeros((len(self.classes),X.shape[0]))\n",
    "        # Define weights as matrix of (n_class, n_features)\n",
    "        self.w_ = np.zeros((len(self.classes),(X.shape[1]+1)))\n",
    "        \n",
    "        # For every class\n",
    "        for cur_class in self.classes:\n",
    "            # Get the indexes of all classes that are positive\n",
    "            idx = y == cur_class\n",
    "            \n",
    "            # Define the positive classes as 1 in a zeroed array\n",
    "            self.y_classes[cur_class][idx] = 1\n",
    "            \n",
    "            # Find the index of the class (for non numerical classes)\n",
    "            idx_class = self.classes.index(cur_class)\n",
    "            \n",
    "            # Create a binary logistic classifier with the parameters passed to the multiclass logistic classifier\n",
    "            self.reg[idx_class] = Regression(self.eta, self.maxIter, self.C, self.optimization, self.regularization)\n",
    "            # Fit all the binary logistic classifiers with the training data and their associated training classes\n",
    "            self.reg[idx_class].fit(X,self.y_classes[cur_class])\n",
    "            \n",
    "            # Copy weights from the binary to the associated location in the multiclass logistic classifer\n",
    "            self.w_[idx_class] = self.reg[idx_class].w_.ravel()\n",
    "            \n",
    "            \n",
    "    def predict(self, X):\n",
    "        # Create a prediction matrix of size (n_classes, n_samples)\n",
    "        prediction = np.zeros((len(self.classes),len(X)))\n",
    "        \n",
    "        # For each binary logistic classifire\n",
    "        for key, reg in self.reg.items():\n",
    "            # Get index of key (for non numerical classes)\n",
    "            idx = self.classes.index(key)\n",
    "            # Predict the raw probability for each sample and make it a 1D array\n",
    "            prediction[key] = reg.predict_raw(X).ravel()\n",
    "            \n",
    "        # Return the row index that has the maximum prediction value\n",
    "        return np.argmax(prediction, axis = 0)\n",
    "    \n",
    "    def predict_raw(self, X):\n",
    "        # Create a prediction matrix of size (n_classes, n_samples)\n",
    "        prediction = np.zeros((len(self.classes),len(X)))\n",
    "        \n",
    "        # For each binary logistic classifire\n",
    "        for key, reg in self.reg.items():\n",
    "            # Get index of key (for non numerical classes)\n",
    "            idx = self.classes.index(key)\n",
    "            # Predict the raw probability for each sample and make it a 1D array\n",
    "            prediction[key] = reg.predict_raw(X).ravel()\n",
    "            \n",
    "        # Return raw prediciton values for each class against each sample\n",
    "        return prediction\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do not remove. This is our best estimation so far\n",
    "multiR_one = MultiClassRegression(eta=1, maxIter=500, C=0.1, opt='GD', reg='NA')\n",
    "multiR_two = MultiClassRegression(eta=1, maxIter=10, C=1, opt='NWT', reg='NA')\n",
    "multiR_three = MultiClassRegression(eta=0.01, maxIter=5, C=0.0001, opt='NWT', reg='L2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jake.rowland/.local/share/virtualenvs/Project4-JwQJWw-L/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/jake.rowland/.local/share/virtualenvs/Project4-JwQJWw-L/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "#Normalize the data to better fit a gradient curve\n",
    "X_predict_norm = X_predict\n",
    "X_train_norm = X_train\n",
    "for col in X_train_norm.columns.values:\n",
    "    mx = X_train_norm[col].max()\n",
    "    X_train_norm[col] = X_train_norm[col]/mx\n",
    "    X_predict_norm[col] = X_predict_norm[col]/mx\n",
    "    \n",
    "#NOT USED CURRENTLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0 0 0 1 2 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0\n",
      " 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 2 0 0 1 0 0 0 1 0 0 0 0 0 1 0\n",
      " 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 1 0 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 2 0 0 0 0 0 0 0 1 1 0 0 1 0 1 1 0 1 1 1 0 2 1 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 1 0 0 1 1 1 0 0 1 0 2 0 2 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0\n",
      " 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1\n",
      " 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0\n",
      " 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0\n",
      " 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 2 0 0 0 0 0 1 0\n",
      " 1 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0\n",
      " 0]\n",
      "[[224 219  19]\n",
      " [ 34  65  24]\n",
      " [  0   1   7]]\n"
     ]
    }
   ],
   "source": [
    "multiR_one.fit(X_train_norm, y_train)\n",
    "pred_one = multiR_one.predict(X_predict_norm)\n",
    "\n",
    "print(pred_one)\n",
    "\n",
    "m = confusion_matrix(pred_one,y_predict)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "[1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 0 2 1 0 1 1 0 1 1\n",
      " 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 1 1 1\n",
      " 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 2\n",
      " 2 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 0 1\n",
      " 1 0 0 1 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 2 0 0 1 1 1\n",
      " 1 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0\n",
      " 0 1 1 1 1 0 1 0 0 0 2 0 1 1 0 0 1 0 1 1 1 0 0 0 1 2 1 1 0 0 0 0 1 0 2 1 0\n",
      " 0 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 1 2 0 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1\n",
      " 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 2 1 1 1 1 0 1 1 0 0\n",
      " 1 1 1 1 0 0 0 1 0 2 0 1 0 1 1 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 0\n",
      " 0 1 0 1 1 1 0 1 0 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 1 1\n",
      " 0 1 0 1 0 0 1 0 0 1 1 0 2 0 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 0 0 2\n",
      " 1 0 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 2 1 1 1 1 1 1 0 1\n",
      " 0 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 0 0 1 0 2 1 0 0 1 0\n",
      " 0 1 1 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0\n",
      " 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1 0\n",
      " 1]\n",
      "[[102 151  39]\n",
      " [151 128   8]\n",
      " [  5   6   3]]\n"
     ]
    }
   ],
   "source": [
    "multiR_two.fit(X_train_norm, y_train)\n",
    "pred_two = multiR_two.predict(X_predict_norm)\n",
    "\n",
    "print(pred_two)\n",
    "\n",
    "m = confusion_matrix(pred_two,y_predict)\n",
    "print(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "[0 2 2 0 2 0 0 2 2 0 0 2 0 2 2 2 2 0 2 0 0 0 0 0 2 0 0 0 2 0 0 0 0 0 0 0 2\n",
      " 0 2 0 2 2 0 0 0 0 0 0 2 0 2 0 2 0 0 0 0 0 2 2 2 2 2 2 2 0 2 2 2 0 0 2 0 0\n",
      " 0 0 2 0 2 0 2 2 2 0 2 0 0 2 0 0 0 0 0 0 0 0 0 0 2 0 0 2 0 2 2 0 2 0 2 0 0\n",
      " 0 2 2 2 2 2 2 0 0 0 2 0 2 0 2 0 0 2 2 2 0 0 0 0 0 0 0 2 0 2 0 2 0 0 0 0 0\n",
      " 0 2 2 0 2 0 0 0 2 2 0 0 0 0 0 2 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 2 2 2 0 2 0\n",
      " 0 0 2 2 2 2 2 0 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 2 2 0 0 0 0 0 2 0\n",
      " 0 0 0 0 0 2 0 2 2 2 2 2 0 0 0 2 0 2 0 0 0 0 2 2 0 2 0 0 2 2 2 0 0 0 0 0 0\n",
      " 0 0 0 2 0 0 0 0 0 2 0 0 0 2 0 2 0 0 2 2 0 2 2 2 2 2 2 0 0 0 0 0 0 0 2 0 0\n",
      " 0 2 2 2 2 0 2 2 2 2 2 2 0 0 0 2 2 2 0 2 2 2 2 2 0 2 0 0 0 0 0 0 0 0 0 2 2\n",
      " 0 0 0 0 2 2 2 0 2 2 2 0 0 0 0 0 2 0 2 0 2 2 0 0 2 0 0 0 0 0 0 0 2 2 0 0 0\n",
      " 2 2 2 0 0 0 2 0 2 0 0 2 0 0 2 2 0 0 0 2 0 2 0 2 0 0 0 0 0 0 0 0 0 0 2 0 0\n",
      " 0 2 2 0 2 2 0 2 0 0 0 0 0 0 0 0 0 2 0 0 2 0 2 0 0 2 0 0 0 1 0 0 2 0 2 2 2\n",
      " 0 2 0 0 0 2 0 0 0 0 2 2 0 0 2 0 0 0 2 2 2 2 0 0 0 0 2 0 0 0 0 2 0 0 0 2 0\n",
      " 2 0 0 0 0 0 0 0 0 2 2 0 2 0 0 0 0 0 2 2 2 2 0 2 2 0 0 0 2 0 2 0 0 2 0 2 0\n",
      " 0 0 2 0 0 0 0 0 2 0 2 2 2 0 0 2 0 0 2 0 0 2 2 0 0 0 0 2 0 2 0 0 2 0 0 2 2\n",
      " 2 0 0 0 0 2 0 0 0 2 0 2 0 2 2 2 2 0 0 0 2 2 0 2 2 2 0 0 2 2 0 2 0 2 0 0 2\n",
      " 0]\n",
      "[[182 163  19]\n",
      " [  0   0   1]\n",
      " [ 76 122  30]]\n"
     ]
    }
   ],
   "source": [
    "multiR_three.fit(X_train_norm, y_train)\n",
    "pred_three = multiR_three.predict(X_predict_norm)\n",
    "\n",
    "print(pred_three)\n",
    "\n",
    "m_three = confusion_matrix(pred_three,y_predict)\n",
    "print(m_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -6.15897934e+39,   6.57544325e+29,  -1.16533550e+28,\n",
       "         -1.37026175e+26,  -8.20796552e+27,  -3.57266759e+27,\n",
       "         -1.10843915e+28,   1.54019168e+27,  -2.06214110e+30,\n",
       "         -4.48954012e+29,  -1.07156152e+29,   2.10295374e+30,\n",
       "          1.15803799e+28,  -9.99531237e+27,   3.21051096e+27,\n",
       "          5.65759085e+39,   5.65759085e+39,   5.01388493e+38,\n",
       "          5.01388493e+38,   5.01388493e+38,   5.01388493e+38],\n",
       "       [ -6.00610322e+40,   2.64093005e+30,   5.48922988e+28,\n",
       "         -4.21708410e+27,  -3.56063312e+28,  -2.63169492e+28,\n",
       "         -3.00824351e+26,   1.27186579e+28,  -8.26453587e+30,\n",
       "         -1.73566991e+30,  -4.53611744e+29,   8.36368926e+30,\n",
       "         -3.23727480e+27,  -5.74983452e+28,   7.87750701e+28,\n",
       "          5.43325724e+40,   5.43325724e+40,   5.72845979e+39,\n",
       "          5.72845979e+39,   5.72845979e+39,   5.72845979e+39],\n",
       "       [ -1.38840436e+43,  -1.34689381e+33,   1.77711742e+31,\n",
       "          2.09323282e+29,   4.27720185e+31,   1.01006360e+31,\n",
       "          1.11569204e+30,  -1.01112378e+31,   8.33143397e+33,\n",
       "          1.77819029e+33,   4.20379099e+32,  -8.49695165e+33,\n",
       "         -1.38519620e+31,   3.27103836e+31,  -5.25399994e+31,\n",
       "          1.43179015e+43,   1.43179015e+43,  -4.33857903e+41,\n",
       "         -4.33857903e+41,  -4.33857903e+41,  -4.33857903e+41]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiR_three.w_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
