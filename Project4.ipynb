{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model\n",
    "\n",
    "matplotlib.rcParams.update({'figure.figsize': (15, 9)})\n",
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "matplotlib.rcParams.update({'axes.labelsize': 20})\n",
    "matplotlib.rcParams.update({'xtick.labelsize': 12})\n",
    "matplotlib.rcParams.update({'ytick.labelsize': 12})\n",
    "matplotlib.rcParams.update({'font.family': 'Helvetica, Arial, sans-serif'})\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning lab 4: extending logistic regression\n",
    "## Jake Rowland and Paul Herz\n",
    "2017-10-01\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "TODO\n",
    "\n",
    "### 1.1 Background\n",
    "\n",
    "TODO\n",
    "\n",
    "### 1.2 Business Case\n",
    "\n",
    "TODO\n",
    "\n",
    "### 1.3 Serviceability\n",
    "\n",
    "TODO\n",
    "\n",
    "## 2. The dataset: preprocessing and review\n",
    "\n",
    "### 2.1 Dataset preparation\n",
    "\n",
    "TODO\n",
    "\n",
    "### 2.2 Data quality\n",
    "\n",
    "TODO\n",
    "\n",
    "## 3. Something\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_title</th>\n",
       "      <th>title_year</th>\n",
       "      <th>genres</th>\n",
       "      <th>plot_keywords</th>\n",
       "      <th>duration</th>\n",
       "      <th>budget</th>\n",
       "      <th>gross</th>\n",
       "      <th>language</th>\n",
       "      <th>country</th>\n",
       "      <th>content_rating</th>\n",
       "      <th>...</th>\n",
       "      <th>director_facebook_likes</th>\n",
       "      <th>actor_1_facebook_likes</th>\n",
       "      <th>actor_2_facebook_likes</th>\n",
       "      <th>actor_3_facebook_likes</th>\n",
       "      <th>cast_total_facebook_likes</th>\n",
       "      <th>movie_imdb_link</th>\n",
       "      <th>num_user_for_reviews</th>\n",
       "      <th>num_critic_for_reviews</th>\n",
       "      <th>num_voted_users</th>\n",
       "      <th>imdb_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Avatar</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>Action|Adventure|Fantasy|Sci-Fi</td>\n",
       "      <td>avatar|future|marine|native|paraplegic</td>\n",
       "      <td>178.0</td>\n",
       "      <td>237000000.0</td>\n",
       "      <td>760505847.0</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>855.0</td>\n",
       "      <td>4834</td>\n",
       "      <td>http://www.imdb.com/title/tt0499549/?ref_=fn_t...</td>\n",
       "      <td>3054.0</td>\n",
       "      <td>723.0</td>\n",
       "      <td>886204</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>Action|Adventure|Fantasy</td>\n",
       "      <td>goddess|marriage ceremony|marriage proposal|pi...</td>\n",
       "      <td>169.0</td>\n",
       "      <td>300000000.0</td>\n",
       "      <td>309404152.0</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>...</td>\n",
       "      <td>563.0</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>48350</td>\n",
       "      <td>http://www.imdb.com/title/tt0449088/?ref_=fn_t...</td>\n",
       "      <td>1238.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>471220</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spectre</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>Action|Adventure|Thriller</td>\n",
       "      <td>bomb|espionage|sequel|spy|terrorist</td>\n",
       "      <td>148.0</td>\n",
       "      <td>245000000.0</td>\n",
       "      <td>200074175.0</td>\n",
       "      <td>English</td>\n",
       "      <td>UK</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>393.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>11700</td>\n",
       "      <td>http://www.imdb.com/title/tt2379713/?ref_=fn_t...</td>\n",
       "      <td>994.0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>275868</td>\n",
       "      <td>6.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Dark Knight Rises</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Action|Thriller</td>\n",
       "      <td>deception|imprisonment|lawlessness|police offi...</td>\n",
       "      <td>164.0</td>\n",
       "      <td>250000000.0</td>\n",
       "      <td>448130642.0</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>...</td>\n",
       "      <td>22000.0</td>\n",
       "      <td>27000.0</td>\n",
       "      <td>23000.0</td>\n",
       "      <td>23000.0</td>\n",
       "      <td>106759</td>\n",
       "      <td>http://www.imdb.com/title/tt1345836/?ref_=fn_t...</td>\n",
       "      <td>2701.0</td>\n",
       "      <td>813.0</td>\n",
       "      <td>1144337</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Star Wars: Episode VII - The Force Awakens    ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Documentary</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>131.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>143</td>\n",
       "      <td>http://www.imdb.com/title/tt5289954/?ref_=fn_t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         movie_title  title_year  \\\n",
       "0                                            Avatar       2009.0   \n",
       "1          Pirates of the Caribbean: At World's End       2007.0   \n",
       "2                                           Spectre       2015.0   \n",
       "3                             The Dark Knight Rises       2012.0   \n",
       "4  Star Wars: Episode VII - The Force Awakens    ...         NaN   \n",
       "\n",
       "                            genres  \\\n",
       "0  Action|Adventure|Fantasy|Sci-Fi   \n",
       "1         Action|Adventure|Fantasy   \n",
       "2        Action|Adventure|Thriller   \n",
       "3                  Action|Thriller   \n",
       "4                      Documentary   \n",
       "\n",
       "                                       plot_keywords  duration       budget  \\\n",
       "0             avatar|future|marine|native|paraplegic     178.0  237000000.0   \n",
       "1  goddess|marriage ceremony|marriage proposal|pi...     169.0  300000000.0   \n",
       "2                bomb|espionage|sequel|spy|terrorist     148.0  245000000.0   \n",
       "3  deception|imprisonment|lawlessness|police offi...     164.0  250000000.0   \n",
       "4                                                NaN       NaN          NaN   \n",
       "\n",
       "         gross language country content_rating     ...      \\\n",
       "0  760505847.0  English     USA          PG-13     ...       \n",
       "1  309404152.0  English     USA          PG-13     ...       \n",
       "2  200074175.0  English      UK          PG-13     ...       \n",
       "3  448130642.0  English     USA          PG-13     ...       \n",
       "4          NaN      NaN     NaN            NaN     ...       \n",
       "\n",
       "  director_facebook_likes  actor_1_facebook_likes  actor_2_facebook_likes  \\\n",
       "0                     0.0                  1000.0                   936.0   \n",
       "1                   563.0                 40000.0                  5000.0   \n",
       "2                     0.0                 11000.0                   393.0   \n",
       "3                 22000.0                 27000.0                 23000.0   \n",
       "4                   131.0                   131.0                    12.0   \n",
       "\n",
       "  actor_3_facebook_likes cast_total_facebook_likes  \\\n",
       "0                  855.0                      4834   \n",
       "1                 1000.0                     48350   \n",
       "2                  161.0                     11700   \n",
       "3                23000.0                    106759   \n",
       "4                    NaN                       143   \n",
       "\n",
       "                                     movie_imdb_link num_user_for_reviews  \\\n",
       "0  http://www.imdb.com/title/tt0499549/?ref_=fn_t...               3054.0   \n",
       "1  http://www.imdb.com/title/tt0449088/?ref_=fn_t...               1238.0   \n",
       "2  http://www.imdb.com/title/tt2379713/?ref_=fn_t...                994.0   \n",
       "3  http://www.imdb.com/title/tt1345836/?ref_=fn_t...               2701.0   \n",
       "4  http://www.imdb.com/title/tt5289954/?ref_=fn_t...                  NaN   \n",
       "\n",
       "   num_critic_for_reviews  num_voted_users  imdb_score  \n",
       "0                   723.0           886204         7.9  \n",
       "1                   302.0           471220         7.1  \n",
       "2                   602.0           275868         6.8  \n",
       "3                   813.0          1144337         8.5  \n",
       "4                     NaN                8         7.1  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "m = pd.read_csv('movie_metadata.csv')\n",
    "\n",
    "# Reorder the DataFrame to a more intelligent fashion\n",
    "m = m[[\n",
    "    'movie_title','title_year',\n",
    "    'genres', 'plot_keywords', 'duration',\n",
    "    'budget', 'gross',\n",
    "    'language', 'country', 'content_rating',\n",
    "    'color', 'aspect_ratio',\n",
    "    'facenumber_in_poster',\n",
    "    'director_name',\n",
    "    'actor_1_name', 'actor_2_name', 'actor_3_name',\n",
    "    'movie_facebook_likes', 'director_facebook_likes', 'actor_1_facebook_likes', 'actor_2_facebook_likes',\n",
    "    'actor_3_facebook_likes', 'cast_total_facebook_likes',\n",
    "    'movie_imdb_link', 'num_user_for_reviews', 'num_critic_for_reviews', 'num_voted_users',\n",
    "    'imdb_score',\n",
    "]]\n",
    "\n",
    "# Reduce the number of float64 data types for columns that do not need a float64 data type\n",
    "for col in ['title_year','facenumber_in_poster',\n",
    "'movie_facebook_likes','actor_1_facebook_likes','actor_2_facebook_likes',\n",
    "'actor_3_facebook_likes','cast_total_facebook_likes','num_user_for_reviews',\n",
    "'num_critic_for_reviews','num_voted_users']:\n",
    "    m[col] = pd.to_numeric(m[col],downcast='integer')\n",
    "  \n",
    "# Remove all duplicate entries\n",
    "m.drop_duplicates(inplace=True)\n",
    "\n",
    "# Create a copy to perserve the original DataFrame\n",
    "m_original=m.copy()\n",
    "m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove foreign films to solve the normalization problem\n",
    "m = m[m['country'] == 'USA']\n",
    "\n",
    "# Remove items with non-American or non-film rating systems\n",
    "m = m[m['content_rating'].isin(['R','PG-13','PG','G'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize the IMDB score into three classes:\n",
    "# [0%-49%] is Poor, [50%-89%] is Average, [90%-100%] is Good.\n",
    "poor_avg = m['imdb_score'].quantile(.5)\n",
    "avg_good = m['imdb_score'].quantile(.9)\n",
    "m['rating_category'] = pd.cut(m.imdb_score,[0,poor_avg,avg_good,10],labels=['poor','average','good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ['movie_title','plot_keywords','actor_1_name','actor_2_name','actor_3_name',\n",
    "'movie_imdb_link','genres', 'director_name','imdb_score','aspect_ratio','country','language']:\n",
    "    m.drop(c, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical values to category type\n",
    "for col in ['content_rating','color']:\n",
    "    m[col] = m[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where the value is null\n",
    "for col in ['title_year', 'language','country','content_rating',\n",
    "'aspect_ratio','duration', 'color','gross','budget','movie_facebook_likes',\n",
    "'actor_1_facebook_likes','actor_2_facebook_likes','actor_3_facebook_likes',\n",
    "'cast_total_facebook_likes']:\n",
    "    try:\n",
    "        m = m[pd.notnull(m[col])]\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume null review counts are 0\n",
    "for col in ['num_user_for_reviews','num_critic_for_reviews']:\n",
    "    m[col].fillna(value=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume missing face counts are the mean\n",
    "avgFace = round(m['facenumber_in_poster'].mean())\n",
    "m['facenumber_in_poster'].fillna(value=avgFace, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2963 entries, 0 to 5042\n",
      "Data columns (total 21 columns):\n",
      "title_year                   2963 non-null float64\n",
      "duration                     2963 non-null float64\n",
      "budget                       2963 non-null float64\n",
      "gross                        2963 non-null float64\n",
      "facenumber_in_poster         2963 non-null float64\n",
      "movie_facebook_likes         2963 non-null int32\n",
      "director_facebook_likes      2963 non-null float64\n",
      "actor_1_facebook_likes       2963 non-null float64\n",
      "actor_2_facebook_likes       2963 non-null float64\n",
      "actor_3_facebook_likes       2963 non-null float64\n",
      "cast_total_facebook_likes    2963 non-null int32\n",
      "num_user_for_reviews         2963 non-null float64\n",
      "num_critic_for_reviews       2963 non-null float64\n",
      "num_voted_users              2963 non-null int32\n",
      "rating_category              2963 non-null category\n",
      "color_ Black and White       2963 non-null uint8\n",
      "color_Color                  2963 non-null uint8\n",
      "content_rating_G             2963 non-null uint8\n",
      "content_rating_PG            2963 non-null uint8\n",
      "content_rating_PG-13         2963 non-null uint8\n",
      "content_rating_R             2963 non-null uint8\n",
      "dtypes: category(1), float64(11), int32(3), uint8(6)\n",
      "memory usage: 332.9 KB\n"
     ]
    }
   ],
   "source": [
    "# replace color and content_rating with dummies\n",
    "m = pd.get_dummies(m, columns=['color','content_rating'])\n",
    "m.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1456\n",
       "1    1231\n",
       "2     276\n",
       "Name: rating_category, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if m.rating_category.dtype != np.dtype('int8'):\n",
    "    m.rating_category = m.rating_category.cat.codes\n",
    "m.rating_category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=m.drop('rating_category', axis=1, inplace=False)\n",
    "y=np.ravel(m['rating_category'])\n",
    "\n",
    "split_index = int(len(m)*0.8)\n",
    "X_train = X[:split_index]\n",
    "X_predict = X[split_index:]\n",
    "y_train = y[:split_index]\n",
    "y_predict = y[split_index:]\n",
    "\n",
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted 1    369\n",
      "0    200\n",
      "2     24\n",
      "dtype: int64\n",
      "Actual 1    285\n",
      "0    258\n",
      "2     50\n",
      "dtype: int64\n",
      "[[139  60   1]\n",
      " [119 220  30]\n",
      " [  0   5  19]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  2.70670632e-06,   9.29206545e-08,   1.94725569e-08,\n",
       "          4.26882976e-09,   3.03963961e-09,  -7.88845810e-06,\n",
       "         -1.67991398e-06,   1.22319154e-06,   2.05359998e-07,\n",
       "          7.82966274e-07,   3.77450250e-06,   1.93695656e-07,\n",
       "          7.69562936e-08,  -1.52688185e-05,  -6.03054113e-11,\n",
       "          1.40916377e-09,  -5.27423771e-11,  -7.76609739e-11,\n",
       "          1.10956127e-09,   3.69700441e-10],\n",
       "       [ -2.87471915e-06,  -1.04729003e-07,  -6.69589930e-09,\n",
       "          2.53262363e-09,  -2.91174758e-09,   5.04803939e-06,\n",
       "          2.30343391e-07,   3.44911792e-06,   3.21321326e-07,\n",
       "         -6.45138284e-07,   1.41261683e-06,  -2.55820800e-08,\n",
       "          5.51190945e-08,  -1.04373501e-06,   4.22430351e-11,\n",
       "         -1.47854413e-09,  -4.18235734e-11,  -2.87056491e-10,\n",
       "         -1.02639122e-09,  -8.10298029e-11],\n",
       "       [ -1.43006905e-03,  -6.47127613e-05,  -1.86114605e-08,\n",
       "         -4.26360138e-09,  -1.24093552e-06,   7.44396755e-06,\n",
       "          3.44411828e-05,   8.24067608e-05,   1.39134666e-04,\n",
       "         -7.08362555e-05,  -9.98667866e-05,  -9.98054219e-05,\n",
       "         -8.37397313e-05,   1.32276457e-05,  -9.72695238e-09,\n",
       "         -7.03434780e-07,   1.12858754e-08,  -5.40268872e-08,\n",
       "         -2.82263454e-07,  -3.88157267e-07]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic = linear_model.LogisticRegression(multi_class='ovr')\n",
    "logistic.fit(X_train,y_train)\n",
    "C = logistic.predict(X_predict)\n",
    "print('Predicted', pd.Series(C).value_counts())\n",
    "print('Actual', pd.Series(y_predict).value_counts())\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "m = confusion_matrix(C,y_predict)\n",
    "print(m)\n",
    "\n",
    "logistic.coef_\n",
    "\n",
    "#  Default\n",
    "# [[130 156  38]\n",
    "#  [ 85  98  10]\n",
    "#  [ 43  31   2]]\n",
    "\n",
    "# Normalized\n",
    "#[[130 156  38]\n",
    "# [ 85  98  10]\n",
    "# [ 43  31   2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "\n",
    "class Regression:\n",
    "\n",
    "    def __init__(self, eta, maxIter=20, C=0.001, opt='GD', reg ='NA'):\n",
    "        self.eta = eta\n",
    "        self.maxIter = maxIter\n",
    "        self.C = C\n",
    "        self.optimization = opt\n",
    "        self.regularization = reg\n",
    "        \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        # ydiff = y(m,1) - yHat(m,1)\n",
    "        # The ravel of ydiff is 1-D array of size m\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        # X is (m,n+1)\n",
    "        # ydiff is 1-D array of size m. ydiff[:,np.newaxis] is matrix of (m,1)\n",
    "        # X * ydiff[:,np.newaxis] is matrix (m,n+1) where each row is X[m] @ ydiff\n",
    "        # Calculate the horizontal average for each row. Result is 1-D array of size m\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        # Reshape the gradient to be matrix (m,1)\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        \n",
    "        if(self.regularization == 'L1'):\n",
    "            # L1 = C * sum(|w|)\n",
    "            gradient[1:] += self.C * np.sum(abs(self.w_[1:])) \n",
    "        elif(self.regularization == 'L2'):\n",
    "            # L2 = C * sum(|w|^2)\n",
    "            gradient[1:] += self.w_[1:]*self.w_[1:] * self.C\n",
    "            #gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        elif(self.regularization == 'L1/L2'):\n",
    "            # DOES NOT WORK\n",
    "            #Calculate the L1 and L2 regularization\n",
    "            L1 = np.sum(abs(self.w_[1:]))\n",
    "            L2 = self.w_[1:]*self.w_[1:]\n",
    "            # Mutlipy the quotient of L1 and L2 by regularization rate\n",
    "            gradient[1:] += np.divide(L1,L2) * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    def _gradient_descent(self):\n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.maxIter):\n",
    "            # Get the gradient of form (m,1)\n",
    "            gradient = self._get_gradient(self.Xb,self.y)\n",
    "            # Add the associated gradient to the weight to adjust the weights.\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate\n",
    "            \n",
    "            \n",
    "    def _stocastic_descent(self):\n",
    "        for _ in range(self.maxIter):\n",
    "            for m in range(1,self.num_samples):\n",
    "                # Calulate the weights for 1st instance, then 2nd, then ..., then mth\n",
    "                gradient = self._get_gradient(self.Xb[:m], self.y[:m])\n",
    "                self.w_ += gradient*self.eta # multipy by learning rate\n",
    "    \n",
    "            \n",
    "    def _newton_H(self,X,y):\n",
    "        # Get P(y=1|x,w_)\n",
    "        hX = self.predict_proba(X,add_bias=False)\n",
    "        # get P(y=0|x,w_)\n",
    "        one_hX = 1 - hX\n",
    "        \n",
    "        # Multiply together\n",
    "        prob_Mult = hX*one_hX\n",
    "        \n",
    "        # Create matrix to perform average on\n",
    "        H = np.zeros((X.shape[1], X.shape[1]))\n",
    "        for row in range(X.shape[0]):\n",
    "            xi = X[row] # Get the row of X\n",
    "            xi.shape = (X.shape[1],1) #Convert to 2D array\n",
    "            # Get the transpose of the row\n",
    "            xiT = xi.T\n",
    "            \n",
    "            # Create a matrix from x and xT\n",
    "            xMat = xi*xiT\n",
    "            \n",
    "            # Scalar multipy the probibility multiplyer \n",
    "            # and the xMatrix\n",
    "            xMat = prob_Mult[row] * xMat\n",
    "            \n",
    "            # Add the matrix to accumulation matrix\n",
    "            H = H + xMat\n",
    "            \n",
    "        # Divide the accumulation matrix by the number of samples to get average\n",
    "        H = H / X.shape[0]\n",
    "        \n",
    "        return H\n",
    "    \n",
    "        \n",
    "    def _newton_method(self):\n",
    "        for _ in range(self.maxIter):\n",
    "            # Get the hessian matrix (Second derivative)\n",
    "            H = self._newton_H(self.Xb, self.y)\n",
    "            # Get the gradient (First Derivative)\n",
    "            gradient = self._get_gradient(self.Xb, self.y)\n",
    "            try:\n",
    "                # Attempt to inverse the hessian\n",
    "                H_inv = np.linalg.inv(H)\n",
    "            \n",
    "                # Modify the w_(t+1) = w_(t) - (dJ(w_))/(d^2J(w_))\n",
    "                self.w_ -= (H_inv @ gradient)\n",
    "            except:\n",
    "                print('Singularity Matrix')\n",
    "    \n",
    "    # public:\n",
    "    # Takes (m,n) matrix - X. Add bias term to X matrix to create (m,n+1) matrix - Xb\n",
    "    # Calculate the dot product of Xb(m,n+1) and w_(n+1,1) -> result is matrix of (m,1)\n",
    "    # Calculate the sigmoid for each value of the dot product and return a matrix of (m,1)\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X,prob=.5):\n",
    "        return (self.predict_proba(X) >= prob)\n",
    "    \n",
    "    def predict_raw(self,X):\n",
    "        return (self.predict_proba(X))\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.Xb = self._add_bias(X) # add bias term\n",
    "        self.num_samples, self.num_features = self.Xb.shape\n",
    "        self.y = y\n",
    "        \n",
    "        self.w_ = np.zeros((self.num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # Select the optimization process\n",
    "        if(self.optimization == 'GD'):\n",
    "            self._gradient_descent()\n",
    "        elif (self.optimization == 'SGD'):\n",
    "            self._stocastic_descent()\n",
    "        elif (self.optimization == 'NWT'):\n",
    "            self._newton_method()\n",
    "        else:\n",
    "            self._gradient_descent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassRegression:\n",
    "    \n",
    "    def __init__(self, eta, maxIter=20, C=0.001, opt='GD', reg ='NA'):\n",
    "        self.eta = eta\n",
    "        self.maxIter = maxIter\n",
    "        self.C = C\n",
    "        self.optimization = opt\n",
    "        self.regularization = reg\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.true_y = y\n",
    "        \n",
    "        # Find unique classes\n",
    "        self.classes = np.unique(y).tolist()\n",
    "        \n",
    "        #Create dictonary to hold regression class associated with the class as the key\n",
    "        self.reg = {}\n",
    "        \n",
    "        # Define training classes as matrix of (n_class, n_samples)\n",
    "        self.y_classes = np.zeros((len(self.classes),X.shape[0]))\n",
    "        # Define weights as matrix of (n_class, n_features)\n",
    "        self.w_ = np.zeros((len(self.classes),(X.shape[1]+1)))\n",
    "        \n",
    "        # For every class\n",
    "        for cur_class in self.classes:\n",
    "            # Get the indexes of all classes that are positive\n",
    "            idx = y == cur_class\n",
    "            \n",
    "            # Define the positive classes as 1 in a zeroed array\n",
    "            self.y_classes[cur_class][idx] = 1\n",
    "            \n",
    "            # Find the index of the class (for non numerical classes)\n",
    "            idx_class = self.classes.index(cur_class)\n",
    "            \n",
    "            # Create a binary logistic classifier with the parameters passed to the multiclass logistic classifier\n",
    "            self.reg[idx_class] = Regression(self.eta, self.maxIter, self.C, self.optimization, self.regularization)\n",
    "            # Fit all the binary logistic classifiers with the training data and their associated training classes\n",
    "            self.reg[idx_class].fit(X,self.y_classes[cur_class])\n",
    "            \n",
    "            # Copy weights from the binary to the associated location in the multiclass logistic classifer\n",
    "            self.w_[idx_class] = self.reg[idx_class].w_.ravel()\n",
    "            \n",
    "            \n",
    "    def predict(self, X):\n",
    "        # Create a prediction matrix of size (n_classes, n_samples)\n",
    "        prediction = np.zeros((len(self.classes),len(X)))\n",
    "        \n",
    "        # For each binary logistic classifire\n",
    "        for key, reg in self.reg.items():\n",
    "            # Get index of key (for non numerical classes)\n",
    "            idx = self.classes.index(key)\n",
    "            # Predict the raw probability for each sample and make it a 1D array\n",
    "            prediction[key] = reg.predict_raw(X).ravel()\n",
    "            \n",
    "        # Return the row index that has the maximum prediction value\n",
    "        return np.argmax(prediction, axis = 0)\n",
    "    \n",
    "    def predict_raw(self, X):\n",
    "        # Create a prediction matrix of size (n_classes, n_samples)\n",
    "        prediction = np.zeros((len(self.classes),len(X)))\n",
    "        \n",
    "        # For each binary logistic classifire\n",
    "        for key, reg in self.reg.items():\n",
    "            # Get index of key (for non numerical classes)\n",
    "            idx = self.classes.index(key)\n",
    "            # Predict the raw probability for each sample and make it a 1D array\n",
    "            prediction[key] = reg.predict_raw(X).ravel()\n",
    "            \n",
    "        # Return raw prediciton values for each class against each sample\n",
    "        return prediction\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do not remove. This is our best estimation so far\n",
    "# multiR = MultiClassRegression(eta=.01, maxIter=5, C=.1, opt='NWT', reg='L1')\n",
    "multiR = MultiClassRegression(eta=.01, maxIter=5, C=.01, opt='NWT', reg='L1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "[2 0 0 0 0 1 1 0 0 2 0 0 0 0 0 0 0 1 0 1 0 0 1 2 0 1 1 1 0 1 1 0 1 1 0 1 0\n",
      " 1 0 1 0 0 1 1 0 1 0 2 0 0 0 0 0 0 2 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1 2\n",
      " 0 1 0 0 0 2 0 0 0 0 0 1 1 0 1 2 2 2 1 0 2 2 2 2 0 0 1 0 1 0 0 0 0 1 0 1 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 2 1 0 1 0 2 1 0 1 0 2 0 0 2 2 0 1\n",
      " 2 0 0 1 0 1 0 0 0 0 1 1 1 1 0 0 1 1 1 0 2 1 0 1 2 0 1 0 1 1 1 0 0 0 1 0 1\n",
      " 2 1 0 0 0 0 0 0 0 0 0 0 0 1 1 2 0 1 0 1 1 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 0\n",
      " 1 1 1 1 2 0 1 0 0 0 0 0 2 0 2 0 1 0 2 1 1 1 0 0 1 0 2 1 0 0 0 0 1 0 1 0 0\n",
      " 0 1 1 0 1 1 1 1 1 0 2 1 1 0 2 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 1 0 2 0 0 0 1 0 0 0 0 0 0 0 1 1 2 0 2 2 2 0 2 0 0\n",
      " 2 2 1 2 0 0 0 1 0 0 0 0 0 0 2 0 0 2 0 1 0 0 1 2 0 1 1 2 1 0 0 2 0 0 2 0 2\n",
      " 0 0 0 2 1 1 0 1 0 0 0 0 1 2 0 0 0 2 1 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 1\n",
      " 0 0 0 2 0 0 1 0 0 1 0 2 1 0 0 1 0 0 0 0 0 2 0 0 0 0 1 1 2 0 2 1 0 0 0 0 0\n",
      " 1 0 1 2 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 2 2 0 1 2 2 0 2\n",
      " 0 2 1 0 1 0 0 0 2 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 2 0 1 0 1 2 0 1 0 2\n",
      " 0 1 0 1 2 2 1 0 0 1 0 0 0 0 1 0 1 2 0 1 1 0 0 1 1 1 1 0 2 0 2 1 0 1 1 0 0\n",
      " 0 1 1 2 1 0 1 2 1 0 1 0 1 0 0 0 0 1 2 1 0 0 0 0 0 0 2 1 0 0 1 0 1 0 1 1 0\n",
      " 1]\n",
      "[363   1 229]\n",
      "[[130 156  38]\n",
      " [ 85  98  10]\n",
      " [ 43  31   2]]\n",
      "[[ -3.11961446e+27   3.33055961e+17  -5.90259730e+15  -6.94057918e+13\n",
      "   -4.15745637e+15  -1.80960916e+15  -5.61440883e+15   7.80129948e+14\n",
      "   -1.04450507e+18  -2.27401871e+17  -5.42761817e+16   1.06517729e+18\n",
      "    5.86563431e+15  -5.06277408e+15   1.62617146e+15   2.86565374e+27\n",
      "    2.86565374e+27   2.53960714e+26   2.53960714e+26   2.53960714e+26\n",
      "    2.53960714e+26]\n",
      " [ -1.27960497e+28   5.62652206e+17   1.16948471e+16  -8.98453055e+14\n",
      "   -7.58595662e+15  -5.60684654e+15  -6.40908626e+13   2.70971997e+15\n",
      "   -1.76076581e+18  -3.69785828e+17  -9.66423357e+16   1.78189052e+18\n",
      "   -6.89703920e+14  -1.22500673e+16   1.67830901e+16   1.15755969e+28\n",
      "    1.15755969e+28   1.22045282e+27   1.22045282e+27   1.22045282e+27\n",
      "    1.22045282e+27]\n",
      " [ -5.79558752e+29  -5.62231088e+19   7.41818435e+17   8.73773830e+15\n",
      "    1.78542349e+18   4.21628752e+17   4.65721015e+16  -4.22071301e+17\n",
      "    3.47777319e+20   7.42266280e+19   1.75477975e+19  -3.54686489e+20\n",
      "   -5.78219574e+17   1.36542275e+18  -2.19316629e+18   5.97669193e+29\n",
      "    5.97669193e+29  -1.81104404e+28  -1.81104404e+28  -1.81104404e+28\n",
      "   -1.81104404e+28]]\n"
     ]
    }
   ],
   "source": [
    "multiR.fit(X_train, y_train)\n",
    "pred = multiR.predict(X_predict)\n",
    "\n",
    "histo = np.bincount(pred)\n",
    "print(maxes)\n",
    "print(histo)\n",
    "\n",
    "m = confusion_matrix(maxes,y_predict)\n",
    "print(m)\n",
    "\n",
    "\n",
    "print(multiR.w_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jake.rowland/.local/share/virtualenvs/Project4-JwQJWw-L/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/jake.rowland/.local/share/virtualenvs/Project4-JwQJWw-L/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data      [2 1 2 ..., 0 0 0]\n",
      "Training 0 targets [0 0 0 ..., 1 1 1]\n",
      "Training 1 targets [0 1 0 ..., 0 0 0]\n",
      "Training 2 targets [0 0 0 ..., 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#Normalize the data to better fit a gradient curve\n",
    "X_predict_norm = X_predict\n",
    "X_train_norm = X_train\n",
    "for col in X_train_norm.columns.values:\n",
    "    mx = X_train_norm[col].max()\n",
    "    X_train_norm[col] = X_train_norm[col]/mx\n",
    "    X_predict_norm[col] = X_predict_norm[col]/mx\n",
    "    \n",
    "#NOT USED CURRENTLY"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
