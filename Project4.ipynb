{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model\n",
    "\n",
    "matplotlib.rcParams.update({'figure.figsize': (15, 9)})\n",
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "matplotlib.rcParams.update({'axes.labelsize': 20})\n",
    "matplotlib.rcParams.update({'xtick.labelsize': 12})\n",
    "matplotlib.rcParams.update({'ytick.labelsize': 12})\n",
    "matplotlib.rcParams.update({'font.family': 'Helvetica, Arial, sans-serif'})\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning lab 4: extending logistic regression\n",
    "## Jake Rowland and Paul Herz\n",
    "2017-10-01\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Here we return to the dataset of our initial project, [Exploring Table Data](https://github.com/SMU-ML-2017/Project1/blob/master/ML%20Lab%201.ipynb), wherein we graphically analyzed the trends in the [IMDB Top 5000 Movie Dataset](https://www.kaggle.com/deepmatrix/imdb-5000-movie-dataset). One aspect of our analysis in that project centered around dividing the set of films into three groups: **poor**, **average**, and **good**. This was a simplification of IMDB's 10-point rating system, and grouped films into asymmetric quantiles. It allowed us to cut arbitrary lines in the large group of films to illustrate trends among the upper percentile films versus the rest. Here, we will try to use several classification techniques to predict these three classes, and compare the results. One technique will involve a handspun implementation to display the inner workings of such an algorithm.\n",
    "\n",
    "### 1.1 Background\n",
    "\n",
    "*Explain the task and what business-case or use-case it is designed to solve (or designed to investigate). Detail exactly what the classification task is and what parties would be interested in the results.*\n",
    "\n",
    "This dataset represents data compiled entirely by IMDB, whose primary purpose is as a compendium of films. In the previous use of this dataset, we mostly considered how this dataset serves as a representation of trends in American and international cinema. This time, we want to consider the films more in the context of IMDB as an application, and the trends of American film consumers.\n",
    "\n",
    "### 1.2 Business Case\n",
    "\n",
    "Using the same arbitrary classifications of **poor**, **average**, and **good**, whose definitions depend on the quantile groupings of IMDB out-of-10 scores, we want to use film data (less the scores) to predict a film's classification into one of these groups. We believe that such a feature will be useful to IMDB, which is likely to receive a sizeable portion of traffic from visitors looking up brand-new films. However, brand-new films do not have sufficient reviews for IMDB to formulate their composite out-of-ten score, and must display a placeholder until there is consensus among a statistically significant number of film critics. Instead of leaving users dissatisfied when they cannot quickly determine whether they should see the film, or how it is performing, IMDB can classify the movie as \"probably bad,\" \"probably average,\" or \"probably good.\" This is not as a replacement to critic composites, but to fill in the blank before reviews come in.\n",
    "\n",
    "### 1.3 Serviceability\n",
    "\n",
    "TODO\n",
    "\n",
    "## 2. The dataset: preprocessing and review\n",
    "\n",
    "### 2.1 Dataset preparation\n",
    "\n",
    "TODO\n",
    "\n",
    "### 2.2 Data quality\n",
    "\n",
    "TODO\n",
    "\n",
    "## 3. Something\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_title</th>\n",
       "      <th>title_year</th>\n",
       "      <th>genres</th>\n",
       "      <th>plot_keywords</th>\n",
       "      <th>duration</th>\n",
       "      <th>budget</th>\n",
       "      <th>gross</th>\n",
       "      <th>language</th>\n",
       "      <th>country</th>\n",
       "      <th>content_rating</th>\n",
       "      <th>...</th>\n",
       "      <th>director_facebook_likes</th>\n",
       "      <th>actor_1_facebook_likes</th>\n",
       "      <th>actor_2_facebook_likes</th>\n",
       "      <th>actor_3_facebook_likes</th>\n",
       "      <th>cast_total_facebook_likes</th>\n",
       "      <th>movie_imdb_link</th>\n",
       "      <th>num_user_for_reviews</th>\n",
       "      <th>num_critic_for_reviews</th>\n",
       "      <th>num_voted_users</th>\n",
       "      <th>imdb_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Avatar</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>Action|Adventure|Fantasy|Sci-Fi</td>\n",
       "      <td>avatar|future|marine|native|paraplegic</td>\n",
       "      <td>178.0</td>\n",
       "      <td>237000000.0</td>\n",
       "      <td>760505847.0</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>855.0</td>\n",
       "      <td>4834</td>\n",
       "      <td>http://www.imdb.com/title/tt0499549/?ref_=fn_t...</td>\n",
       "      <td>3054.0</td>\n",
       "      <td>723.0</td>\n",
       "      <td>886204</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>Action|Adventure|Fantasy</td>\n",
       "      <td>goddess|marriage ceremony|marriage proposal|pi...</td>\n",
       "      <td>169.0</td>\n",
       "      <td>300000000.0</td>\n",
       "      <td>309404152.0</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>...</td>\n",
       "      <td>563.0</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>48350</td>\n",
       "      <td>http://www.imdb.com/title/tt0449088/?ref_=fn_t...</td>\n",
       "      <td>1238.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>471220</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spectre</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>Action|Adventure|Thriller</td>\n",
       "      <td>bomb|espionage|sequel|spy|terrorist</td>\n",
       "      <td>148.0</td>\n",
       "      <td>245000000.0</td>\n",
       "      <td>200074175.0</td>\n",
       "      <td>English</td>\n",
       "      <td>UK</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>393.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>11700</td>\n",
       "      <td>http://www.imdb.com/title/tt2379713/?ref_=fn_t...</td>\n",
       "      <td>994.0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>275868</td>\n",
       "      <td>6.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Dark Knight Rises</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Action|Thriller</td>\n",
       "      <td>deception|imprisonment|lawlessness|police offi...</td>\n",
       "      <td>164.0</td>\n",
       "      <td>250000000.0</td>\n",
       "      <td>448130642.0</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>...</td>\n",
       "      <td>22000.0</td>\n",
       "      <td>27000.0</td>\n",
       "      <td>23000.0</td>\n",
       "      <td>23000.0</td>\n",
       "      <td>106759</td>\n",
       "      <td>http://www.imdb.com/title/tt1345836/?ref_=fn_t...</td>\n",
       "      <td>2701.0</td>\n",
       "      <td>813.0</td>\n",
       "      <td>1144337</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Star Wars: Episode VII - The Force Awakens    ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Documentary</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>131.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>143</td>\n",
       "      <td>http://www.imdb.com/title/tt5289954/?ref_=fn_t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         movie_title  title_year  \\\n",
       "0                                            Avatar       2009.0   \n",
       "1          Pirates of the Caribbean: At World's End       2007.0   \n",
       "2                                           Spectre       2015.0   \n",
       "3                             The Dark Knight Rises       2012.0   \n",
       "4  Star Wars: Episode VII - The Force Awakens    ...         NaN   \n",
       "\n",
       "                            genres  \\\n",
       "0  Action|Adventure|Fantasy|Sci-Fi   \n",
       "1         Action|Adventure|Fantasy   \n",
       "2        Action|Adventure|Thriller   \n",
       "3                  Action|Thriller   \n",
       "4                      Documentary   \n",
       "\n",
       "                                       plot_keywords  duration       budget  \\\n",
       "0             avatar|future|marine|native|paraplegic     178.0  237000000.0   \n",
       "1  goddess|marriage ceremony|marriage proposal|pi...     169.0  300000000.0   \n",
       "2                bomb|espionage|sequel|spy|terrorist     148.0  245000000.0   \n",
       "3  deception|imprisonment|lawlessness|police offi...     164.0  250000000.0   \n",
       "4                                                NaN       NaN          NaN   \n",
       "\n",
       "         gross language country content_rating     ...      \\\n",
       "0  760505847.0  English     USA          PG-13     ...       \n",
       "1  309404152.0  English     USA          PG-13     ...       \n",
       "2  200074175.0  English      UK          PG-13     ...       \n",
       "3  448130642.0  English     USA          PG-13     ...       \n",
       "4          NaN      NaN     NaN            NaN     ...       \n",
       "\n",
       "  director_facebook_likes  actor_1_facebook_likes  actor_2_facebook_likes  \\\n",
       "0                     0.0                  1000.0                   936.0   \n",
       "1                   563.0                 40000.0                  5000.0   \n",
       "2                     0.0                 11000.0                   393.0   \n",
       "3                 22000.0                 27000.0                 23000.0   \n",
       "4                   131.0                   131.0                    12.0   \n",
       "\n",
       "  actor_3_facebook_likes cast_total_facebook_likes  \\\n",
       "0                  855.0                      4834   \n",
       "1                 1000.0                     48350   \n",
       "2                  161.0                     11700   \n",
       "3                23000.0                    106759   \n",
       "4                    NaN                       143   \n",
       "\n",
       "                                     movie_imdb_link num_user_for_reviews  \\\n",
       "0  http://www.imdb.com/title/tt0499549/?ref_=fn_t...               3054.0   \n",
       "1  http://www.imdb.com/title/tt0449088/?ref_=fn_t...               1238.0   \n",
       "2  http://www.imdb.com/title/tt2379713/?ref_=fn_t...                994.0   \n",
       "3  http://www.imdb.com/title/tt1345836/?ref_=fn_t...               2701.0   \n",
       "4  http://www.imdb.com/title/tt5289954/?ref_=fn_t...                  NaN   \n",
       "\n",
       "   num_critic_for_reviews  num_voted_users  imdb_score  \n",
       "0                   723.0           886204         7.9  \n",
       "1                   302.0           471220         7.1  \n",
       "2                   602.0           275868         6.8  \n",
       "3                   813.0          1144337         8.5  \n",
       "4                     NaN                8         7.1  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "m = pd.read_csv('movie_metadata.csv')\n",
    "\n",
    "# Reorder the DataFrame to a more intelligent fashion\n",
    "m = m[[\n",
    "    'movie_title','title_year',\n",
    "    'genres', 'plot_keywords', 'duration',\n",
    "    'budget', 'gross',\n",
    "    'language', 'country', 'content_rating',\n",
    "    'color', 'aspect_ratio',\n",
    "    'facenumber_in_poster',\n",
    "    'director_name',\n",
    "    'actor_1_name', 'actor_2_name', 'actor_3_name',\n",
    "    'movie_facebook_likes', 'director_facebook_likes', 'actor_1_facebook_likes', 'actor_2_facebook_likes',\n",
    "    'actor_3_facebook_likes', 'cast_total_facebook_likes',\n",
    "    'movie_imdb_link', 'num_user_for_reviews', 'num_critic_for_reviews', 'num_voted_users',\n",
    "    'imdb_score',\n",
    "]]\n",
    "\n",
    "# Reduce the number of float64 data types for columns that do not need a float64 data type\n",
    "for col in ['title_year','facenumber_in_poster',\n",
    "'movie_facebook_likes','actor_1_facebook_likes','actor_2_facebook_likes',\n",
    "'actor_3_facebook_likes','cast_total_facebook_likes','num_user_for_reviews',\n",
    "'num_critic_for_reviews','num_voted_users']:\n",
    "    m[col] = pd.to_numeric(m[col],downcast='integer')\n",
    "  \n",
    "# Remove all duplicate entries\n",
    "m.drop_duplicates(inplace=True)\n",
    "\n",
    "# Create a copy to perserve the original DataFrame\n",
    "m_original=m.copy()\n",
    "m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove foreign films to solve the normalization problem\n",
    "m = m[m['country'] == 'USA']\n",
    "\n",
    "# Remove items with non-American or non-film rating systems\n",
    "m = m[m['content_rating'].isin(['R','PG-13','PG','G'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize the IMDB score into three classes:\n",
    "# [0%-49%] is Poor, [50%-89%] is Average, [90%-100%] is Good.\n",
    "poor_avg = m['imdb_score'].quantile(.5)\n",
    "avg_good = m['imdb_score'].quantile(.9)\n",
    "m['rating_category'] = pd.cut(m.imdb_score,[0,poor_avg,avg_good,10],labels=['poor','average','good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ['movie_title','plot_keywords','actor_1_name','actor_2_name','actor_3_name',\n",
    "'movie_imdb_link','genres', 'director_name','imdb_score','aspect_ratio','country','language']:\n",
    "    m.drop(c, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical values to category type\n",
    "for col in ['content_rating','color']:\n",
    "    m[col] = m[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where the value is null\n",
    "for col in ['title_year', 'language','country','content_rating',\n",
    "'aspect_ratio','duration', 'color','gross','budget','movie_facebook_likes',\n",
    "'actor_1_facebook_likes','actor_2_facebook_likes','actor_3_facebook_likes',\n",
    "'cast_total_facebook_likes']:\n",
    "    try:\n",
    "        m = m[pd.notnull(m[col])]\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume null review counts are 0\n",
    "for col in ['num_user_for_reviews','num_critic_for_reviews']:\n",
    "    m[col].fillna(value=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume missing face counts are the mean\n",
    "avgFace = round(m['facenumber_in_poster'].mean())\n",
    "m['facenumber_in_poster'].fillna(value=avgFace, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2963 entries, 0 to 5042\n",
      "Data columns (total 21 columns):\n",
      "title_year                   2963 non-null float64\n",
      "duration                     2963 non-null float64\n",
      "budget                       2963 non-null float64\n",
      "gross                        2963 non-null float64\n",
      "facenumber_in_poster         2963 non-null float64\n",
      "movie_facebook_likes         2963 non-null int32\n",
      "director_facebook_likes      2963 non-null float64\n",
      "actor_1_facebook_likes       2963 non-null float64\n",
      "actor_2_facebook_likes       2963 non-null float64\n",
      "actor_3_facebook_likes       2963 non-null float64\n",
      "cast_total_facebook_likes    2963 non-null int32\n",
      "num_user_for_reviews         2963 non-null float64\n",
      "num_critic_for_reviews       2963 non-null float64\n",
      "num_voted_users              2963 non-null int32\n",
      "rating_category              2963 non-null category\n",
      "color_ Black and White       2963 non-null uint8\n",
      "color_Color                  2963 non-null uint8\n",
      "content_rating_G             2963 non-null uint8\n",
      "content_rating_PG            2963 non-null uint8\n",
      "content_rating_PG-13         2963 non-null uint8\n",
      "content_rating_R             2963 non-null uint8\n",
      "dtypes: category(1), float64(11), int32(3), uint8(6)\n",
      "memory usage: 332.9 KB\n"
     ]
    }
   ],
   "source": [
    "# replace color and content_rating with dummies\n",
    "m = pd.get_dummies(m, columns=['color','content_rating'])\n",
    "m.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1456\n",
       "1    1231\n",
       "2     276\n",
       "Name: rating_category, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if m.rating_category.dtype != np.dtype('int8'):\n",
    "    m.rating_category = m.rating_category.cat.codes\n",
    "m.rating_category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=m.drop('rating_category', axis=1, inplace=False)\n",
    "y=np.ravel(m['rating_category'])\n",
    "\n",
    "split_index = int(len(m)*0.8)\n",
    "X_train = X[:split_index]\n",
    "X_predict = X[split_index:]\n",
    "y_train = y[:split_index]\n",
    "y_predict = y[split_index:]\n",
    "\n",
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted 1    369\n",
      "0    200\n",
      "2     24\n",
      "dtype: int64\n",
      "Actual 1    285\n",
      "0    258\n",
      "2     50\n",
      "dtype: int64\n",
      "[[139  60   1]\n",
      " [119 220  30]\n",
      " [  0   5  19]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  2.70670632e-06,   9.29206545e-08,   1.94725562e-08,\n",
       "          4.26883064e-09,   3.03963961e-09,  -7.88845810e-06,\n",
       "         -1.67991398e-06,   1.22319154e-06,   2.05359998e-07,\n",
       "          7.82966274e-07,   3.77450250e-06,   1.93695656e-07,\n",
       "          7.69562936e-08,  -1.52688185e-05,  -6.03054113e-11,\n",
       "          1.40916377e-09,  -5.27423771e-11,  -7.76609739e-11,\n",
       "          1.10956127e-09,   3.69700441e-10],\n",
       "       [ -2.87471917e-06,  -1.04729004e-07,  -6.69589931e-09,\n",
       "          2.53262363e-09,  -2.91174761e-09,   5.04803943e-06,\n",
       "          2.30343394e-07,   3.44911795e-06,   3.21321331e-07,\n",
       "         -6.45138288e-07,   1.41261685e-06,  -2.55820801e-08,\n",
       "          5.51190949e-08,  -1.04373501e-06,   4.22430355e-11,\n",
       "         -1.47854414e-09,  -4.18235738e-11,  -2.87056494e-10,\n",
       "         -1.02639123e-09,  -8.10298036e-11],\n",
       "       [ -1.43173340e-03,  -6.47588820e-05,  -1.81908834e-08,\n",
       "         -4.42616069e-09,  -1.24294533e-06,   7.06268752e-06,\n",
       "          3.56130656e-05,   7.91656450e-05,   1.39059951e-04,\n",
       "         -7.03459678e-05,  -1.02271532e-04,  -9.99571956e-05,\n",
       "         -8.38765143e-05,   1.35085118e-05,  -9.70402664e-09,\n",
       "         -7.04283245e-07,   1.13752742e-08,  -5.39198695e-08,\n",
       "         -2.82858715e-07,  -3.88583961e-07]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic = linear_model.LogisticRegression(multi_class='ovr')\n",
    "logistic.fit(X_train,y_train)\n",
    "C = logistic.predict(X_predict)\n",
    "print('Predicted', pd.Series(C).value_counts())\n",
    "print('Actual', pd.Series(y_predict).value_counts())\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "m = confusion_matrix(C,y_predict)\n",
    "print(m)\n",
    "\n",
    "logistic.coef_\n",
    "\n",
    "#  Default\n",
    "# [[130 156  38]\n",
    "#  [ 85  98  10]\n",
    "#  [ 43  31   2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "\n",
    "class Regression:\n",
    "\n",
    "    def __init__(self, eta, maxIter=20, C=0.001, opt='GD', reg ='NA'):\n",
    "        self.eta = eta\n",
    "        self.maxIter = maxIter\n",
    "        self.C = C\n",
    "        self.optimization = opt\n",
    "        self.regularization = reg\n",
    "        \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        # ydiff = y(m,1) - yHat(m,1)\n",
    "        # The ravel of ydiff is 1-D array of size m\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        # X is (m,n+1)\n",
    "        # ydiff is 1-D array of size m. ydiff[:,np.newaxis] is matrix of (m,1)\n",
    "        # X * ydiff[:,np.newaxis] is matrix (m,n+1) where each row is X[m] @ ydiff\n",
    "        # Calculate the horizontal average for each row. Result is 1-D array of size m\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        # Reshape the gradient to be matrix (m,1)\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        \n",
    "        if(self.regularization == 'L1'):\n",
    "            # L1 = C * sum(|w|)\n",
    "            gradient[1:] += self.C * np.sum(abs(self.w_[1:])) \n",
    "        elif(self.regularization == 'L2'):\n",
    "            # L2 = C * sum(|w|^2)\n",
    "            gradient[1:] += np.sum(self.w_[1:]*self.w_[1:]) * self.C\n",
    "            #gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        elif(self.regularization == 'L1/L2'):\n",
    "            # DOES NOT WORK\n",
    "            #Calculate the L1 and L2 regularization\n",
    "            L1 = np.sum(abs(self.w_[1:]))\n",
    "            L2 = self.w_[1:]*self.w_[1:]\n",
    "            # Mutlipy the quotient of L1 and L2 by regularization rate\n",
    "            gradient[1:] += np.divide(L1,L2) * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    def _gradient_descent(self):\n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.maxIter):\n",
    "            # Get the gradient of form (m,1)\n",
    "            gradient = self._get_gradient(self.Xb,self.y)\n",
    "            # Add the associated gradient to the weight to adjust the weights.\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate\n",
    "            \n",
    "            \n",
    "    def _stocastic_descent(self):\n",
    "        for _ in range(self.maxIter):\n",
    "            for m in range(1,self.num_samples):\n",
    "                # Calulate the weights for 1st instance, then first 2, then first 3..., then first m instances\n",
    "                gradient = self._get_gradient(self.Xb[:m], self.y[:m])\n",
    "                self.w_ += gradient*self.eta # multipy by learning rate\n",
    "    \n",
    "            \n",
    "    def _newton_H(self,X,y):\n",
    "        # Get P(y=1|x,w_)\n",
    "        hX = self.predict_proba(X,add_bias=False)\n",
    "        # get P(y=0|x,w_)\n",
    "        one_hX = 1 - hX\n",
    "        \n",
    "        # Multiply together\n",
    "        prob_Mult = hX*one_hX\n",
    "        \n",
    "        # Create matrix to perform average on\n",
    "        H = np.zeros((X.shape[1], X.shape[1]))\n",
    "        for row in range(X.shape[0]):\n",
    "            xi = X[row] # Get the row of X\n",
    "            xi.shape = (X.shape[1],1) #Convert to 2D array\n",
    "            # Get the transpose of the row\n",
    "            xiT = xi.T\n",
    "            \n",
    "            # Create a matrix from x and xT\n",
    "            xMat = xi*xiT\n",
    "            \n",
    "            # Scalar multipy the probibility multiplyer \n",
    "            # and the xMatrix\n",
    "            xMat = prob_Mult[row] * xMat\n",
    "            \n",
    "            # Add the matrix to accumulation matrix\n",
    "            H = H + xMat\n",
    "            \n",
    "        # Divide the accumulation matrix by the number of samples to get average\n",
    "        H = H / X.shape[0]\n",
    "        \n",
    "        return H\n",
    "    \n",
    "        \n",
    "    def _newton_method(self):\n",
    "        for _ in range(self.maxIter):\n",
    "            # Get the hessian matrix (Second derivative)\n",
    "            H = self._newton_H(self.Xb, self.y)\n",
    "            # Get the gradient (First Derivative)\n",
    "            gradient = self._get_gradient(self.Xb, self.y)\n",
    "            try:\n",
    "                # Attempt to inverse the hessian\n",
    "                H_inv = np.linalg.inv(H)\n",
    "            \n",
    "                # Modify the w_(t+1) = w_(t) - (dJ(w_))/(d^2J(w_))\n",
    "                self.w_ -= (H_inv @ gradient)\n",
    "            except:\n",
    "                print('Singularity Matrix')\n",
    "    \n",
    "    # public:\n",
    "    # Takes (m,n) matrix - X. Add bias term to X matrix to create (m,n+1) matrix - Xb\n",
    "    # Calculate the dot product of Xb(m,n+1) and w_(n+1,1) -> result is matrix of (m,1)\n",
    "    # Calculate the sigmoid for each value of the dot product and return a matrix of (m,1)\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X,prob=.5):\n",
    "        return (self.predict_proba(X) >= prob)\n",
    "    \n",
    "    def predict_raw(self,X):\n",
    "        return (self.predict_proba(X))\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.Xb = self._add_bias(X) # add bias term\n",
    "        self.num_samples, self.num_features = self.Xb.shape\n",
    "        self.y = y\n",
    "        \n",
    "        self.w_ = np.zeros((self.num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # Select the optimization process\n",
    "        if(self.optimization == 'GD'):\n",
    "            self._gradient_descent()\n",
    "        elif (self.optimization == 'SGD'):\n",
    "            self._stocastic_descent()\n",
    "        elif (self.optimization == 'NWT'):\n",
    "            self._newton_method()\n",
    "        else:\n",
    "            self._gradient_descent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassRegression:\n",
    "    \n",
    "    def __init__(self, eta, maxIter=20, C=0.001, opt='GD', reg ='NA'):\n",
    "        self.eta = eta\n",
    "        self.maxIter = maxIter\n",
    "        self.C = C\n",
    "        self.optimization = opt\n",
    "        self.regularization = reg\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.true_y = y\n",
    "        \n",
    "        # Find unique classes\n",
    "        self.classes = np.unique(y).tolist()\n",
    "        \n",
    "        #Create dictonary to hold regression class associated with the class as the key\n",
    "        self.reg = {}\n",
    "        \n",
    "        # Define training classes as matrix of (n_class, n_samples)\n",
    "        self.y_classes = np.zeros((len(self.classes),X.shape[0]))\n",
    "        # Define weights as matrix of (n_class, n_features)\n",
    "        self.w_ = np.zeros((len(self.classes),(X.shape[1]+1)))\n",
    "        \n",
    "        # For every class\n",
    "        for cur_class in self.classes:\n",
    "            # Get the indexes of all classes that are positive\n",
    "            idx = y == cur_class\n",
    "            \n",
    "            # Define the positive classes as 1 in a zeroed array\n",
    "            self.y_classes[cur_class][idx] = 1\n",
    "            \n",
    "            # Find the index of the class (for non numerical classes)\n",
    "            idx_class = self.classes.index(cur_class)\n",
    "            \n",
    "            # Create a binary logistic classifier with the parameters passed to the multiclass logistic classifier\n",
    "            self.reg[idx_class] = Regression(self.eta, self.maxIter, self.C, self.optimization, self.regularization)\n",
    "            # Fit all the binary logistic classifiers with the training data and their associated training classes\n",
    "            self.reg[idx_class].fit(X,self.y_classes[cur_class])\n",
    "            \n",
    "            # Copy weights from the binary to the associated location in the multiclass logistic classifer\n",
    "            self.w_[idx_class] = self.reg[idx_class].w_.ravel()\n",
    "            \n",
    "            \n",
    "    def predict(self, X):\n",
    "        # Create a prediction matrix of size (n_classes, n_samples)\n",
    "        prediction = np.zeros((len(self.classes),len(X)))\n",
    "        \n",
    "        # For each binary logistic classifire\n",
    "        for key, reg in self.reg.items():\n",
    "            # Get index of key (for non numerical classes)\n",
    "            idx = self.classes.index(key)\n",
    "            # Predict the raw probability for each sample and make it a 1D array\n",
    "            prediction[key] = reg.predict_raw(X).ravel()\n",
    "            \n",
    "        # Return the row index that has the maximum prediction value\n",
    "        return np.argmax(prediction, axis = 0)\n",
    "    \n",
    "    def predict_raw(self, X):\n",
    "        # Create a prediction matrix of size (n_classes, n_samples)\n",
    "        prediction = np.zeros((len(self.classes),len(X)))\n",
    "        \n",
    "        # For each binary logistic classifire\n",
    "        for key, reg in self.reg.items():\n",
    "            # Get index of key (for non numerical classes)\n",
    "            idx = self.classes.index(key)\n",
    "            # Predict the raw probability for each sample and make it a 1D array\n",
    "            prediction[key] = reg.predict_raw(X).ravel()\n",
    "            \n",
    "        # Return raw prediciton values for each class against each sample\n",
    "        return prediction\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do not remove. This is our best estimation so far\n",
    "multiR_one = MultiClassRegression(eta=1, maxIter=500, C=0.1, opt='GD', reg='NA')\n",
    "multiR_two = MultiClassRegression(eta=1, maxIter=10, C=1, opt='NWT', reg='NA')\n",
    "multiR_three = MultiClassRegression(eta=0.01, maxIter=5, C=0.0001, opt='NWT', reg='L2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jake.rowland/.local/share/virtualenvs/Project4-JwQJWw-L/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/jake.rowland/.local/share/virtualenvs/Project4-JwQJWw-L/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "#Normalize the data to better fit a gradient curve\n",
    "X_predict_norm = X_predict\n",
    "X_train_norm = X_train\n",
    "for col in X_train_norm.columns.values:\n",
    "    mx = X_train_norm[col].max()\n",
    "    X_train_norm[col] = X_train_norm[col]/mx\n",
    "    X_predict_norm[col] = X_predict_norm[col]/mx\n",
    "    \n",
    "#NOT USED CURRENTLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0 0 0 1 2 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0\n",
      " 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 2 0 0 1 0 0 0 1 0 0 0 0 0 1 0\n",
      " 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 1 0 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 2 0 0 0 0 0 0 0 1 1 0 0 1 0 1 1 0 1 1 1 0 2 1 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 1 0 0 1 1 1 0 0 1 0 2 0 2 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0\n",
      " 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1\n",
      " 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0\n",
      " 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0\n",
      " 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 2 0 0 0 0 0 1 0\n",
      " 1 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0\n",
      " 0]\n",
      "[[224 219  19]\n",
      " [ 34  65  24]\n",
      " [  0   1   7]]\n"
     ]
    }
   ],
   "source": [
    "multiR_one.fit(X_train_norm, y_train)\n",
    "pred_one = multiR_one.predict(X_predict_norm)\n",
    "\n",
    "print(pred_one)\n",
    "\n",
    "m = confusion_matrix(pred_one,y_predict)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "[1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 0 2 1 0 1 1 0 1 1\n",
      " 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 1 1 1\n",
      " 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 2\n",
      " 2 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 0 1\n",
      " 1 0 0 1 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 2 0 0 1 1 1\n",
      " 1 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0\n",
      " 0 1 1 1 1 0 1 0 0 0 2 0 1 1 0 0 1 0 1 1 1 0 0 0 1 2 1 1 0 0 0 0 1 0 2 1 0\n",
      " 0 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 1 2 0 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1\n",
      " 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 2 1 1 1 1 0 1 1 0 0\n",
      " 1 1 1 1 0 0 0 1 0 2 0 1 0 1 1 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 0\n",
      " 0 1 0 1 1 1 0 1 0 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 1 1\n",
      " 0 1 0 1 0 0 1 0 0 1 1 0 2 0 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 0 0 2\n",
      " 1 0 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 2 1 1 1 1 1 1 0 1\n",
      " 0 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 0 0 1 0 2 1 0 0 1 0\n",
      " 0 1 1 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0\n",
      " 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1 0\n",
      " 1]\n",
      "[[102 151  39]\n",
      " [151 128   8]\n",
      " [  5   6   3]]\n"
     ]
    }
   ],
   "source": [
    "multiR_two.fit(X_train_norm, y_train)\n",
    "pred_two = multiR_two.predict(X_predict_norm)\n",
    "\n",
    "print(pred_two)\n",
    "\n",
    "m = confusion_matrix(pred_two,y_predict)\n",
    "print(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "Singularity Matrix\n",
      "[0 2 2 0 2 0 0 2 2 0 0 2 0 2 2 2 2 0 2 0 0 0 0 0 2 0 0 0 2 0 0 0 0 0 0 0 2\n",
      " 0 2 0 2 2 0 0 0 0 0 0 2 0 2 0 2 0 0 0 0 0 2 2 2 2 2 2 2 0 2 2 2 0 0 2 0 0\n",
      " 0 0 2 0 2 0 2 2 2 0 2 0 0 2 0 0 0 0 0 0 0 0 0 0 2 0 0 2 0 2 2 0 2 0 2 0 0\n",
      " 0 2 2 2 2 2 2 0 0 0 2 0 2 0 2 0 0 2 2 2 0 0 0 0 0 0 0 2 0 2 0 2 0 0 0 0 0\n",
      " 0 2 2 0 2 0 0 0 2 2 0 0 0 0 0 2 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 2 2 2 0 2 0\n",
      " 0 0 2 2 2 2 2 0 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 2 2 0 0 0 0 0 2 0\n",
      " 0 0 0 0 0 2 0 2 2 2 2 2 0 0 0 2 0 2 0 0 0 0 2 2 0 2 0 0 2 2 2 0 0 0 0 0 0\n",
      " 0 0 0 2 0 0 0 0 0 2 0 0 0 2 0 2 0 0 2 2 0 2 2 2 2 2 2 0 0 0 0 0 0 0 2 0 0\n",
      " 0 2 2 2 2 0 2 2 2 2 2 2 0 0 0 2 2 2 0 2 2 2 2 2 0 2 0 0 0 0 0 0 0 0 0 2 2\n",
      " 0 0 0 0 2 2 2 0 2 2 2 0 0 0 0 0 2 0 2 0 2 2 0 0 2 0 0 0 0 0 0 0 2 2 0 0 0\n",
      " 2 2 2 0 0 0 2 0 2 0 0 2 0 0 2 2 0 0 0 2 0 2 0 2 0 0 0 0 0 0 0 0 0 0 2 0 0\n",
      " 0 2 2 0 2 2 0 2 0 0 0 0 0 0 0 0 0 2 0 0 2 0 2 0 0 2 0 0 0 1 0 0 2 0 2 2 2\n",
      " 0 2 0 0 0 2 0 0 0 0 2 2 0 0 2 0 0 0 2 2 2 2 0 0 0 0 2 0 0 0 0 2 0 0 0 2 0\n",
      " 2 0 0 0 0 0 0 0 0 2 2 0 2 0 0 0 0 0 2 2 2 2 0 2 2 0 0 0 2 0 2 0 0 2 0 2 0\n",
      " 0 0 2 0 0 0 0 0 2 0 2 2 2 0 0 2 0 0 2 0 0 2 2 0 0 0 0 2 0 2 0 0 2 0 0 2 2\n",
      " 2 0 0 0 0 2 0 0 0 2 0 2 0 2 2 2 2 0 0 0 2 2 0 2 2 2 0 0 2 2 0 2 0 2 0 0 2\n",
      " 0]\n",
      "[[182 163  19]\n",
      " [  0   0   1]\n",
      " [ 76 122  30]]\n"
     ]
    }
   ],
   "source": [
    "multiR_three.fit(X_train_norm, y_train)\n",
    "pred_three = multiR_three.predict(X_predict_norm)\n",
    "\n",
    "print(pred_three)\n",
    "\n",
    "m_three = confusion_matrix(pred_three,y_predict)\n",
    "print(m_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -6.15897934e+39,   6.57544325e+29,  -1.16533550e+28,\n",
       "         -1.37026175e+26,  -8.20796552e+27,  -3.57266759e+27,\n",
       "         -1.10843915e+28,   1.54019168e+27,  -2.06214110e+30,\n",
       "         -4.48954012e+29,  -1.07156152e+29,   2.10295374e+30,\n",
       "          1.15803799e+28,  -9.99531237e+27,   3.21051096e+27,\n",
       "          5.65759085e+39,   5.65759085e+39,   5.01388493e+38,\n",
       "          5.01388493e+38,   5.01388493e+38,   5.01388493e+38],\n",
       "       [ -6.00610322e+40,   2.64093005e+30,   5.48922988e+28,\n",
       "         -4.21708410e+27,  -3.56063312e+28,  -2.63169492e+28,\n",
       "         -3.00824351e+26,   1.27186579e+28,  -8.26453587e+30,\n",
       "         -1.73566991e+30,  -4.53611744e+29,   8.36368926e+30,\n",
       "         -3.23727480e+27,  -5.74983452e+28,   7.87750701e+28,\n",
       "          5.43325724e+40,   5.43325724e+40,   5.72845979e+39,\n",
       "          5.72845979e+39,   5.72845979e+39,   5.72845979e+39],\n",
       "       [ -1.38840436e+43,  -1.34689381e+33,   1.77711742e+31,\n",
       "          2.09323282e+29,   4.27720185e+31,   1.01006360e+31,\n",
       "          1.11569204e+30,  -1.01112378e+31,   8.33143397e+33,\n",
       "          1.77819029e+33,   4.20379099e+32,  -8.49695165e+33,\n",
       "         -1.38519620e+31,   3.27103836e+31,  -5.25399994e+31,\n",
       "          1.43179015e+43,   1.43179015e+43,  -4.33857903e+41,\n",
       "         -4.33857903e+41,  -4.33857903e+41,  -4.33857903e+41]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiR_three.w_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
